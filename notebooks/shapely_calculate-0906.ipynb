{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58db4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapleyValues:\n",
    "    '''For storing and plotting Shapley values.'''\n",
    "    def __init__(self, values, std):\n",
    "        self.values = values\n",
    "        self.std = std\n",
    "\n",
    "    def plot(self,\n",
    "             feature_names=None,\n",
    "             sort_features=True,\n",
    "             max_features=np.inf,\n",
    "             orientation='horizontal',\n",
    "             error_bars=True,\n",
    "             color='C0',\n",
    "             title='Feature Importance',\n",
    "             title_size=20,\n",
    "             tick_size=16,\n",
    "             tick_rotation=None,\n",
    "             axis_label='',\n",
    "             label_size=16,\n",
    "             figsize=(10, 7),\n",
    "             return_fig=False):\n",
    "        '''\n",
    "        Plot Shapley values.\n",
    "\n",
    "        Args:\n",
    "          feature_names: list of feature names.\n",
    "          sort_features: whether to sort features by their Shapley values.\n",
    "          max_features: number of features to display.\n",
    "          orientation: horizontal (default) or vertical.\n",
    "          error_bars: whether to include standard deviation error bars.\n",
    "          color: bar chart color.\n",
    "          title: plot title.\n",
    "          title_size: font size for title.\n",
    "          tick_size: font size for feature names and numerical values.\n",
    "          tick_rotation: tick rotation for feature names (vertical plots only).\n",
    "          label_size: font size for label.\n",
    "          figsize: figure size (if fig is None).\n",
    "          return_fig: whether to return matplotlib figure object.\n",
    "        '''\n",
    "        return plotting.plot(\n",
    "            self, feature_names, sort_features, max_features, orientation,\n",
    "            error_bars, color, title, title_size, tick_size, tick_rotation,\n",
    "            axis_label, label_size, figsize, return_fig)\n",
    "\n",
    "def default_min_variance_samples(game):\n",
    "    '''Determine min_variance_samples.'''\n",
    "    return 5\n",
    "\n",
    "def default_variance_batches(num_players, batch_size):\n",
    "    '''\n",
    "    Determine variance_batches.\n",
    "\n",
    "    This value tries to ensure that enough samples are included to make A\n",
    "    approximation non-singular.\n",
    "    '''\n",
    "\n",
    "    return int(np.ceil(10 * num_players / batch_size))\n",
    "\n",
    "def calculate_result(A, b, total):\n",
    "    '''Calculate the regression coefficients.'''\n",
    "    num_players = A.shape[1]\n",
    "    try:\n",
    "        if len(b.shape) == 2:\n",
    "            A_inv_one = np.linalg.solve(A, np.ones((num_players, 1)))\n",
    "        else:\n",
    "            A_inv_one = np.linalg.solve(A, np.ones(num_players))\n",
    "        A_inv_vec = np.linalg.solve(A, b)\n",
    "        values = (\n",
    "            A_inv_vec -\n",
    "            A_inv_one * (np.sum(A_inv_vec, axis=0, keepdims=True) - total)\n",
    "            / np.sum(A_inv_one))\n",
    "    except np.linalg.LinAlgError:\n",
    "        raise ValueError('singular matrix inversion. Consider using larger '\n",
    "                         'variance_batches')\n",
    "\n",
    "    return values\n",
    "\n",
    "def ShapleyRegressionPrecomputed(\n",
    "                      grand_value,\n",
    "                      null_value,\n",
    "                      model_outputs,\n",
    "                      masks, \n",
    "                      num_players,\n",
    "                      batch_size=512,\n",
    "                      detect_convergence=True,\n",
    "                      thresh=0.01,\n",
    "                      n_samples=None,\n",
    "                      paired_sampling=True,\n",
    "                      return_all=False,\n",
    "                      min_variance_samples=None,\n",
    "                      variance_batches=None,\n",
    "                      bar=True,\n",
    "                      verbose=False):\n",
    "    # Verify arguments.\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    if min_variance_samples is None:\n",
    "        min_variance_samples = 5\n",
    "    else:\n",
    "        assert isinstance(min_variance_samples, int)\n",
    "        assert min_variance_samples > 1\n",
    "\n",
    "    if variance_batches is None:\n",
    "        variance_batches = default_variance_batches(num_players, batch_size)\n",
    "    else:\n",
    "        assert isinstance(variance_batches, int)\n",
    "        assert variance_batches >= 1\n",
    "\n",
    "    # Possibly force convergence detection.\n",
    "    if n_samples is None:\n",
    "        n_samples = 1e20\n",
    "        if not detect_convergence:\n",
    "            detect_convergence = True\n",
    "            if verbose:\n",
    "                print('Turning convergence detection on')\n",
    "\n",
    "    if detect_convergence:\n",
    "        assert 0 < thresh < 1\n",
    "\n",
    "    # Weighting kernel (probability of each subset size).\n",
    "    weights = np.arange(1, num_players)\n",
    "    weights = 1 / (weights * (num_players - weights))\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # Calculate null and grand coalitions for constraints.\n",
    "    null = null_value\n",
    "    grand = grand_value\n",
    "\n",
    "    # Calculate difference between grand and null coalitions.\n",
    "    total = grand - null\n",
    "\n",
    "    # Set up bar.\n",
    "    n_loops = int(np.ceil(n_samples / batch_size))\n",
    "    if bar:\n",
    "        if detect_convergence:\n",
    "            bar = tqdm(total=1)\n",
    "        else:\n",
    "            bar = tqdm(total=n_loops * batch_size)\n",
    "\n",
    "    # Setup.\n",
    "    n = 0\n",
    "    b = 0\n",
    "    A = 0\n",
    "    estimate_list = []\n",
    "\n",
    "    # For variance estimation.\n",
    "    A_sample_list = []\n",
    "    b_sample_list = []\n",
    "\n",
    "    # For tracking progress.\n",
    "    var = np.nan * np.ones(num_players)\n",
    "    if return_all:\n",
    "        N_list = []\n",
    "        std_list = []\n",
    "        val_list = []\n",
    "\n",
    "    # Begin sampling.\n",
    "    for it in range(n_loops):\n",
    "        # Sample subsets.\n",
    "        #print(subsets.shape)\n",
    "        S=masks[batch_size*it:batch_size*(it+1)]\n",
    "        game_S=model_outputs[batch_size*it:batch_size*(it+1)]\n",
    "#         print(\"S\", S, S.sum(axis=1))\n",
    "#         print(\"game(s)\", game_S)\n",
    "#         print(\"game(s)-null\", game_S-null)\n",
    "\n",
    "\n",
    "        A_sample = np.matmul(S[:, :, np.newaxis].astype(float),\n",
    "                             S[:, np.newaxis, :].astype(float))\n",
    "\n",
    "\n",
    "        b_sample = (S.astype(float).T\n",
    "                    * (game_S - null)[:, np.newaxis].T).T\n",
    "        \n",
    "#         print(\"b\", b_sample)\n",
    "#         print(\"variance_batches\", variance_batches)\n",
    "\n",
    "        # Welford's algorithm.\n",
    "        n += batch_size\n",
    "        b += np.sum(b_sample - b, axis=0) / n\n",
    "        A += np.sum(A_sample - A, axis=0) / n\n",
    "\n",
    "        # Calculate progress.\n",
    "        values = calculate_result(A, b, total)\n",
    "        A_sample_list.append(A_sample)\n",
    "        b_sample_list.append(b_sample)\n",
    "        if len(A_sample_list) == variance_batches:\n",
    "            # Aggregate samples for intermediate estimate.\n",
    "            A_sample = np.concatenate(A_sample_list, axis=0).mean(axis=0)\n",
    "            b_sample = np.concatenate(b_sample_list, axis=0).mean(axis=0)\n",
    "            A_sample_list = []\n",
    "            b_sample_list = []\n",
    "\n",
    "            # Add new estimate.\n",
    "            estimate_list.append(calculate_result(A_sample, b_sample, total))\n",
    "\n",
    "            # Estimate current var.\n",
    "            # print(len(estimate_list), min_variance_samples)\n",
    "            if len(estimate_list) >= min_variance_samples:\n",
    "                var = np.array(estimate_list).var(axis=0)\n",
    "\n",
    "        # Convergence ratio.\n",
    "        std = np.sqrt(var * variance_batches / (it + 1))\n",
    "        ratio = np.max(\n",
    "            np.max(std, axis=0) / (values.max(axis=0) - values.min(axis=0)))\n",
    "        # print(\"std\", var)\n",
    "        # Print progress message.\n",
    "        if verbose:\n",
    "            if detect_convergence:\n",
    "                print(f'StdDev Ratio = {ratio:.4f} (Converge at {thresh:.4f})')\n",
    "            else:\n",
    "                print(f'StdDev Ratio = {ratio:.4f}')\n",
    "\n",
    "        # Check for convergence.\n",
    "        if detect_convergence:\n",
    "            if ratio < thresh:\n",
    "                if verbose:\n",
    "                    print('Detected convergence')\n",
    "\n",
    "                # Skip bar ahead.\n",
    "                if bar:\n",
    "                    bar.n = bar.total\n",
    "                    bar.refresh()\n",
    "                break\n",
    "\n",
    "        # Forecast number of iterations required.\n",
    "        if detect_convergence:\n",
    "            N_est = (it + 1) * (ratio / thresh) ** 2\n",
    "            if bar and not np.isnan(N_est):\n",
    "                bar.n = np.around((it + 1) / N_est, 4)\n",
    "                bar.refresh()\n",
    "        elif bar:\n",
    "            bar.update(batch_size)\n",
    "\n",
    "        # Save intermediate quantities.\n",
    "        if return_all:\n",
    "            val_list.append(values)\n",
    "            std_list.append(std)\n",
    "            if detect_convergence:\n",
    "                N_list.append(N_est)\n",
    "        \n",
    "        # print(\"size\", batch_size*it, len(masks))\n",
    "        if batch_size*(it+1)>=len(masks):\n",
    "            break\n",
    "    print(ratio)\n",
    "    # Return results.\n",
    "    if return_all:\n",
    "        # Dictionary for progress tracking.\n",
    "        iters = (\n",
    "            (np.arange(it + 1) + 1) * batch_size *\n",
    "            (1))\n",
    "        tracking_dict = {\n",
    "            'values': val_list,\n",
    "            'std': std_list,\n",
    "            'iters': iters}\n",
    "        if detect_convergence:\n",
    "            tracking_dict['N_est'] = N_list\n",
    "\n",
    "        return ShapleyValues(values, std), tracking_dict\n",
    "    else:\n",
    "        return ShapleyValues(values, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_eval_results(path):\n",
    "    file_set=set([p for p in glob.glob(str(Path(path)/\"*.pt\")) if p.split('/')[-1]!=\"shapley_output.pt\"])\n",
    "    \n",
    "    path_grand_null=str(Path(path)/\"grand_null.pt\")\n",
    "    file_set.remove(path_grand_null)\n",
    "    \n",
    "    file_list=sorted(list(file_set), key=lambda x: x.split(\"_\")[-2])\n",
    "    begin_idx=int(file_list[0].split('_')[-2])\n",
    "    end_idx=int(file_list[0].split('_')[-1].replace('.pt',''))\n",
    "    step_size=end_idx-begin_idx\n",
    "    \n",
    "    idx=begin_idx\n",
    "    \n",
    "    path_eval_list=[]\n",
    "    while True:\n",
    "        path_eval=str(Path(path)/f\"mask_eval_{idx}_{idx+step_size}.pt\")\n",
    "        if path_eval in file_set:\n",
    "            file_set.remove(path_eval)\n",
    "            path_eval_list.append(path_eval)\n",
    "        else:\n",
    "            break\n",
    "        idx+=step_size\n",
    "            \n",
    "    assert len(file_set)==0\n",
    "    \n",
    "    grand_null=torch.load(path_grand_null)\n",
    "    eval_list=[torch.load(path_eval) for path_eval in path_eval_list]\n",
    "    \n",
    "    grand_logits=grand_null[\"logits\"][0]\n",
    "    grand_masks=grand_null[\"masks\"][0]\n",
    "    null_logits=grand_null[\"logits\"][1]\n",
    "    nulll_masks=grand_null[\"masks\"][1]\n",
    "    \n",
    "    eval_logits=np.concatenate([eval_value[\"logits\"] for eval_value in eval_list], axis=0)\n",
    "    eval_masks=np.concatenate([eval_value[\"masks\"] for eval_value in eval_list], axis=0)\n",
    "    \n",
    "    return {\n",
    "        \"grand\": {\"logits\": grand_logits, \"masks\": grand_masks},\n",
    "        \"null\": {\"logits\": null_logits, \"masks\": nulll_masks},\n",
    "        \"subsets\": {\"logits\": eval_logits, \"masks\": eval_masks},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d0de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "     \"\"\"Parse the command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Process some inputs.')\n",
    "    \n",
    "    # Argument for batch size without default\n",
    "    parser.add_argument('--batch_size', type=int, required=True,\n",
    "                        help='The batch size for processing.')\n",
    "    \n",
    "    # Argument for input path without default\n",
    "    parser.add_argument('--input_path', type=str, required=True,\n",
    "                        help='Path to the input directory.')\n",
    "    \n",
    "    # Argument for normalization function\n",
    "    parser.add_argument('--normalize_function', type=str, choices=['softmax'], required=True,\n",
    "                        help='The normalization function to be used. Options: softmax.')\n",
    "    \n",
    "    parser.add_argument('--num_players', type=int, required=True,\n",
    "                        help='The number of players')    \n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args = get_args()\n",
    "    \n",
    "    # Accessing the arguments\n",
    "    batch_size = args.batch_size\n",
    "    input_path = args.input_path\n",
    "    if args.normalize_function == \"softmax\":\n",
    "        normalize_function = softmax\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported normalization function\")\n",
    "    num_players = args.num_players\n",
    "\n",
    "        \n",
    "    sample_list=glob.glob(str(Path(input_path)/\"[0-9]*\"))\n",
    "    \n",
    "    pbar=tqdm(sample_list)\n",
    "    \n",
    "    for sample_path in pbar:\n",
    "        eval_results=read_eval_results(path=sample_path)\n",
    "\n",
    "        grand_value=eval_results[\"grand\"][\"logits\"]\n",
    "        if len(grand_value.shape)==1:\n",
    "            grand_value=partial(normalize_function, axis=0)(grand_value)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Not supported grand shape {grand_value.shape}\")\n",
    "\n",
    "        null_value=eval_results[\"null\"][\"logits\"]\n",
    "        if len(null_value.shape)==1:\n",
    "            null_value=partial(normalize_function, axis=0)(null_value)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Not supported null shape {null_value.shape}\")       \n",
    "\n",
    "        subsets_output=eval_results[\"subsets\"][\"logits\"]\n",
    "        if len(subsets_output.shape)==1:\n",
    "            subsets_output=partial(normalize_function, axis=1)(subsets_output)    \n",
    "        else:\n",
    "            raise RuntimeError(f\"Not supported subset outputs shape {subsets_output.shape}\")        \n",
    "                               \n",
    "    \n",
    "        subsets=eval_results[\"subsets\"][\"masks\"]\n",
    "        \n",
    "        assert subsets_output.shape[1]==grand_value.shape[0]==null_value.shape[0]\n",
    "        assert subsets.shape[1]==num_players        \n",
    "\n",
    "        _,tracking_dict=ShapleyRegressionPrecomputed(grand_value=grand_value,\n",
    "                      null_value=null_value,\n",
    "                      model_outputs=subsets_output,\n",
    "                      masks=subsets,\n",
    "                      batch_size=batch_size,\n",
    "                      num_players=num_players,\n",
    "                      variance_batches=2,\n",
    "                      min_variance_samples=2,\n",
    "                      return_all=True,\n",
    "                      bar=False\n",
    "                      )  \n",
    "\n",
    "        torch.save(obj=tracking_dict, f=str(Path(sample_path)/\"shapley_output.pt\"))\n",
    "        \n",
    "        pbar.set_postfix(ratio,refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parent=\"logs/vitbase_imagenette_surrogate_eval_test/extract_output/test/*\"\n",
    "print(len(glob.glob(path_parent)))\n",
    "iter_prev=None\n",
    "for idx, path in enumerate(glob.glob(path_parent)):\n",
    "    loaded=torch.load(path+'/shapley_output.pt')\n",
    "    \n",
    "    if iter_prev is not None and iter_prev!=loaded[\"iters\"][-1]:\n",
    "        print(idx, iter_prev, loaded[\"iters\"])\n",
    "    \n",
    "    if iter_prev is None:\n",
    "        print(loaded[\"iters\"])\n",
    "    iter_prev=loaded[\"iters\"][-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638831c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded[\"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3370a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded[\"iters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abf2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
